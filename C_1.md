# S-3: Fundamentals of Optimization Problems

# C-1: Optimization Fundamentals and Hill Climbing

1. Introduction to Optimization
   - Fundamentals of Optimization in AI
   - Problem Types and Complexity (P vs NP)
   - Objective Functions and State Spaces
   - Global vs Local Optima Challenges
   - Randomized Approaches for NP-Hard Problems
2. Hill Climbing Algorithm
   - Fundamental Principles of Greedy Local Search
   - Algorithm Implementation and Pseudocode
   - Local Maxima, Plateaus, and Ridges Problems
   - Random Restart Technique
   - Step Size Configuration Strategies
3. Local Beam Search
   - Multiple Simultaneous State Exploration
   - Information Sharing Across Parallel Searches
   - Regular vs Stochastic Beam Search
   - Implementation Challenges and Solutions
   - Applications in Complex Landscapes

#### Introduction to Optimization

##### Fundamentals of Optimization in AI

Optimization lies at the heart of artificial intelligence, representing our attempt to find the most effective solution
among many alternatives. While traditional search algorithms focus on finding any valid path to a goal, optimization
algorithms aim to find the best possible solution according to specific criteria.

At its core, optimization involves defining an objective function (sometimes called a fitness function, cost function,
or utility function) that measures the quality of a proposed solution. Depending on the context, we may want to either
maximize this function (when it represents value, utility, or profit) or minimize it (when it represents cost, error, or
distance).

The mathematical form of an optimization problem can be expressed as:

$$\text{Find } x^* \text{ such that } f(x^*) \leq f(x) \text{ for all } x \in S$$

Where:

- $x^*$ is the optimal solution
- $f(x)$ is the objective function to be minimized
- $S$ is the search space (all possible solutions)

For maximization problems, we simply negate the objective function or reverse the inequality.

Optimization algorithms appear throughout AI applications:

- Training neural networks by minimizing prediction error
- Finding the shortest path in navigation systems
- Allocating resources efficiently in planning problems
- Scheduling tasks to minimize completion time
- Configuring system parameters for optimal performance

The challenge of optimization comes from the vast size of the solution space, which makes exhaustive search impractical
for most interesting problems.

##### Problem Types and Complexity (P vs NP)

Optimization problems can be categorized by their computational complexity, which describes how challenging they are to
solve as the problem size increases.

Problems in class P (Polynomial time) can be solved in time proportional to a polynomial function of the input size. For
example, sorting a list of n numbers can be done in O(n log n) time, making it tractable even for large inputs.

NP (Nondeterministic Polynomial time) problems are those where a proposed solution can be verified quickly (in
polynomial time), but finding the solution may require exponential time in the worst case. The classic example is the
Boolean satisfiability problem: given a logical formula, find variable assignments that make it true.

The relationship between P and NP represents one of computer science's greatest unsolved questions: are these classes
actually different, or is P = NP?

Many optimization problems fall into the category of NP-hard, meaning they're at least as difficult as the hardest
problems in NP. Examples include:

- The Traveling Salesman Problem (finding the shortest route that visits all cities exactly once)
- Graph coloring (assigning colors to vertices so no adjacent vertices share colors)
- Knapsack problem (selecting items with maximum value while staying within a weight constraint)

When facing NP-hard problems, we often need to employ approximation algorithms or heuristics to find reasonably good
(but not guaranteed optimal) solutions in acceptable time.

##### Objective Functions and State Spaces

The objective function (also called the evaluation function) quantifies how "good" a particular solution is. It maps
each potential solution to a numerical value, creating a landscape that we can visualize and navigate.

For example:

- In the Traveling Salesman Problem, the objective function is the total distance of the route
- In neural network training, it's the prediction error on training data
- In portfolio optimization, it's the expected return given risk constraints

The state space represents all possible solutions to our problem. We can visualize this as a landscape where:

- Each point represents a complete solution
- The height at each point represents the solution's quality
- Our goal is to find the highest peak (for maximization) or lowest valley (for minimization)

The structure of this landscape dramatically affects how difficult optimization becomes:

- Smooth, convex landscapes with a single optimum allow for gradient-based methods that efficiently reach the global
  optimum
- Rugged landscapes with many local optima require more sophisticated techniques to avoid getting trapped in suboptimal
  solutions

##### Global vs Local Optima Challenges

A fundamental challenge in optimization is distinguishing between local and global optima:

- A global optimum is the absolute best solution across the entire search space
- A local optimum is a solution that's better than all its neighbors, but not necessarily the best overall

Many simple optimization algorithms can get trapped in local optima, mistaking them for the best possible solution. This
is analogous to climbing a mountain in fog—you might reach what seems like the summit, only to discover later that
you've climbed a minor peak while the true summit lies elsewhere.


Different optimization landscapes present various challenges:

- Local optima: Points that are better than all neighboring points but worse than the global optimum
- Plateaus: Flat regions where many neighboring states have identical values
- Ridges: Narrow paths of increasing value with steep dropoffs on either side
- Valleys: For minimization problems, these represent areas of low objective function values

The presence of multiple local optima makes optimization especially challenging, as simple "greedy" algorithms that
always move toward improvement will get stuck.

##### Randomized Approaches for NP-Hard Problems

Given the challenges of NP-hard optimization problems, randomization offers a powerful approach. Rather than
exhaustively searching the entire solution space (which would be computationally infeasible), randomized algorithms
inject controlled randomness to explore promising regions.

Key randomized approaches include:

1. **Random Restart**: When stuck in a local optimum, simply restart the search from a new random starting point. After
   many restarts, keep the best solution found.
2. **Stochastic Local Search**: Make random moves in the solution space, but with a bias toward moves that improve the
   objective function.
3. **Simulated Annealing**: Inspired by metallurgical annealing, this algorithm occasionally accepts worse solutions
   with a probability that decreases over time, allowing escape from local optima.
4. **Genetic Algorithms**: Mimicking natural selection, these maintain a population of solutions, combine good solutions
   to produce "offspring," and occasionally introduce random mutations.
5. **Monte Carlo Methods**: Use random sampling to estimate properties of the solution space.

The power of randomization is that it helps algorithms escape local optima while still focusing computation on promising
areas of the search space. By balancing exploration (trying new areas) with exploitation (refining good solutions),
randomized approaches can find high-quality solutions to problems that would otherwise be intractable.

This concept is fundamental to many modern optimization techniques in AI, including the optimization algorithms we'll
explore in this course: hill climbing, simulated annealing, and genetic algorithms.

#### Hill Climbing Algorithm

##### Fundamental Principles of Greedy Local Search

Hill climbing is one of the simplest optimization techniques, embodying a straightforward principle: always move uphill.
This algorithm belongs to the family of local search methods that explore the immediate neighborhood of the current
solution to find improvements.

The core philosophy behind hill climbing is greediness – at each step, the algorithm selects the neighboring state that
offers the greatest improvement in the objective function. This approach is analogous to a hiker climbing a mountain in
dense fog who can only see a few feet in any direction. The sensible strategy in such conditions is to always step in
the direction that leads upward most steeply.

Hill climbing operates with no memory of past states or foresight into future states. It focuses solely on the current
position and its immediate neighbors, making decisions based on local information only. This myopic view has both
advantages and disadvantages:

Advantages:

- Simple to implement and understand
- Requires minimal computational resources
- Works well for convex problems with a single peak
- Often finds good solutions quickly in the early stages

The greedy nature of hill climbing makes it particularly suitable for problems where the evaluation function provides a
smooth gradient toward the optimal solution. It transforms the complex task of global optimization into a series of
simple, local decisions.

##### Algorithm Implementation and Pseudocode

Implementing hill climbing requires defining three key components:

1. A representation of the solution state
2. A method to generate neighboring states
3. An objective function to evaluate solutions

Here's the pseudocode for a basic hill climbing algorithm:

```python
function HILL-CLIMBING(problem) returns a state that is a local maximum
    current ← MAKE-NODE(problem.INITIAL-STATE)
    loop do
        neighbor ← a highest-valued successor of current
        if neighbor.VALUE ≤ current.VALUE then
            return current.STATE
        current ← neighbor
```

Let's walk through a simple example of hill climbing applied to the 8-Queens problem, where we need to place eight
queens on a chessboard so that no queen attacks another:

1. Start with a random placement of 8 queens on the board
2. Evaluate the current state by counting pairs of non-attacking queens
3. Generate neighboring states by moving a single queen within its column
4. Select the neighbor with the fewest attacking pairs
5. If no neighbor is better than the current state, return the current state as a local optimum
6. Otherwise, move to the best neighbor and repeat

The implementation can be enhanced with various strategies to handle different problem characteristics, such as using
different neighborhood generation methods or incorporating adaptive step sizes.

##### Local Maxima, Plateaus, and Ridges Problems

Despite its simplicity, hill climbing suffers from three significant limitations that arise from its greedy, local
perspective:

1. **Local Maxima**: A local maximum is a peak that is higher than all its neighboring states but lower than the global
   maximum. Because hill climbing only considers local improvements, it will stop at the first local maximum it
   encounters, even if better solutions exist elsewhere in the search space.

   For example, in the 8-Queens problem, the algorithm might find a configuration with just one pair of queens
   attacking each other and be unable to find a better move, despite the existence of a perfect solution with no
   attacks.

2. **Plateaus**: A plateau is a flat region in the search space where neighboring states have identical values. When the
   algorithm reaches a plateau, it cannot determine which direction leads to improvement, potentially causing it to
   wander aimlessly or terminate prematurely.

   In the context of job scheduling, a plateau might occur when multiple different task orderings yield the same
   completion time, providing no guidance on which direction to explore.

3. **Ridges**: A ridge is a sequence of states that is better than neighboring states but requires the algorithm to make
   moves that temporarily decrease value before reaching a higher peak. Standard hill climbing cannot navigate ridges
   because it never accepts downward moves.

   For instance, in protein folding simulations, the algorithm might need to temporarily increase the energy state (a
   seemingly worse solution) to cross an energy barrier before finding the lowest-energy configuration.

These limitations explain why basic hill climbing, though simple and intuitive, often proves insufficient for complex
problems with rugged optimization landscapes.

##### Random Restart Technique

A surprisingly effective enhancement to hill climbing is the random restart approach. Rather than accepting the first
local maximum found, the algorithm performs multiple independent searches from different random starting points,
ultimately keeping the best solution found across all attempts.

The random restart technique works as follows:

1. Run hill climbing until it reaches a local maximum
2. Record this solution if it's the best found so far
3. Restart the search from a new, randomly generated starting state
4. Repeat steps 1-3 for a predefined number of iterations or until a satisfactory solution is found
5. Return the best solution discovered across all restarts

This approach transforms hill climbing from an incomplete algorithm (one that might not find the optimal solution) to a
probabilistically complete one—given enough restarts, the probability of finding the global optimum approaches 1.

The effectiveness of random restarts comes from the observation that many optimization landscapes have numerous local
optima, but with varying "basin sizes" (the regions of the search space that lead to a particular optimum). By sampling
many starting points, we increase our chances of landing in the basin of the global optimum or at least a high-quality
local optimum.

For the 8-Queens problem, experiments show that random restart hill climbing typically finds a solution after just a few
restarts, making it a practical approach despite the large search space.

##### Step Size Configuration Strategies

The performance of hill climbing can be significantly improved by carefully configuring the step size—how far the
algorithm moves in the search space at each iteration. The ideal step size depends on the characteristics of the problem
and changes as the search progresses.

Several strategies for step size configuration include:

1. **Fixed Step Size**: The simplest approach uses a constant step size throughout the search. This works well when the
   scale of the search space is known and consistent.
2. **Decreasing Step Size**: Start with large steps to quickly move toward promising regions, then gradually decrease
   the step size for fine-tuning. This mimics the common practice in gradient-based optimization of reducing the
   learning rate over time.
3. **Adaptive Step Size**: Adjust the step size based on progress. If recent steps have yielded significant
   improvements, maintain or increase the step size; if improvements have been small, reduce the step size to enable
   more precise movements.
4. **Stochastic Step Size**: Introduce randomness in the step size, occasionally taking larger jumps to escape local
   optima or plateaus.
5. **Problem-Specific Step Sizes**: Tailor the step size to the structure of the problem. For example, in continuous
   spaces, steps might be proportional to the gradient magnitude, while in discrete spaces, steps might involve changing
   more elements of the solution as the search progresses.

A well-tuned step size configuration can dramatically improve hill climbing's ability to navigate complex landscapes,
helping it balance the tradeoff between exploration (covering more of the search space) and exploitation (refining
promising solutions).

In practice, implementing these strategies often transforms the basic hill climbing algorithm into more sophisticated
variants such as stochastic hill climbing, simulated annealing, or tabu search, which we'll explore in later sections.

#### Local Beam Search

##### Multiple Simultaneous State Exploration

Local beam search extends the hill climbing algorithm by exploring multiple states in parallel rather than focusing on a
single state. While hill climbing maintains and improves just one candidate solution at a time, local beam search keeps
track of k different states simultaneously, where k is called the "beam width." This parallel exploration provides a
broader view of the search space.

The fundamental insight behind local beam search is that exploring multiple paths increases the probability of finding
the global optimum or at least a high-quality solution. If one path leads to a local optimum, other paths might avoid it
and discover better solutions elsewhere in the search landscape.

The process begins by randomly generating k initial states. In each iteration, the algorithm evaluates all the immediate
neighbors of all k current states. From this combined pool of successor states, it selects the k best states to maintain
for the next iteration. This selection is based on the objective function value, keeping only the most promising
candidates.

For example, in a robot path planning problem, local beam search might simultaneously explore several different routes,
periodically discarding the least promising paths while focusing computational resources on the most efficient ones.
This parallel exploration provides robustness against getting trapped in dead ends or suboptimal routes.

The value of k presents an important trade-off: larger beam widths explore more of the search space simultaneously but
require more computational resources. Smaller beam widths are more efficient but may not provide sufficient diversity to
escape local optima.

##### Information Sharing Across Parallel Searches

A key advantage of local beam search over simply running multiple independent hill climbing instances is the information
sharing that occurs between parallel search paths. This collaboration emerges from the selection process, where the k
best states from the entire pool of successors are chosen regardless of which parent states generated them.

Information sharing creates an implicit form of communication between different search paths:

1. **Success Propagation**: When one search path discovers a particularly promising region, the algorithm naturally
   allocates more resources to explore that region in subsequent iterations.
2. **Diversity Maintenance**: Although the algorithm selects the k best states, these states are often distributed
   across different regions of the search space, maintaining diversity in the exploration.
3. **Implicit Memory**: The collection of k states serves as a form of memory about promising regions discovered so far.
4. **Computational Efficiency**: By focusing on the k most promising states at each step, the algorithm allocates
   computational resources more effectively than independent searches would.

This collaborative aspect is particularly valuable in problems with deceptive landscapes, where the path to the global
optimum might initially appear less promising than paths leading to local optima. By maintaining multiple diverse
candidates, local beam search can discover routes that individual hill climbing instances might miss.

However, information sharing can sometimes be counterproductive. If the k states become too similar (converging to the
same region of the search space), the beam search loses its diversity advantage and may behave similarly to a single
hill climbing instance.

##### Regular vs Stochastic Beam Search

Local beam search comes in two main variants: regular (or deterministic) beam search and stochastic beam search. These
approaches differ in how they select the k states to maintain at each iteration.

**Regular Beam Search** selects the k states with the highest objective function values from among all successors. This
greedy selection ensures that computation focuses on the most promising states discovered so far. However, this
deterministic approach can lead to a problem known as premature convergence, where all k states cluster in the same
region of the search space, effectively reducing the search to a single path.

For instance, in a job scheduling optimization problem, regular beam search might quickly focus all k states on
schedules that prioritize the most time-critical tasks first. While this is often sensible, it might miss creative
solutions that temporarily delay some critical tasks to achieve better overall scheduling.

**Stochastic Beam Search** introduces randomness into the selection process. Instead of deterministically choosing the k
best successors, it selects successors probabilistically, with selection probability proportional to the objective
function value. States with higher values are more likely to be selected, but lower-valued states still have a chance,
maintaining diversity in the search.

The probabilistic selection can be implemented using techniques like fitness-proportionate selection (roulette wheel
selection) or tournament selection borrowed from genetic algorithms. The amount of randomness can be tuned through
parameters like temperature in softmax selection.

Stochastic beam search is particularly valuable in rugged search landscapes with many local optima. The controlled
randomness helps the algorithm explore diverse regions of the search space while still focusing computational resources
on promising areas.

##### Implementation Challenges and Solutions

Implementing local beam search effectively requires addressing several challenges:

1. **Maintaining Diversity**: One of the most common issues is premature convergence, where all k states become
   concentrated in the same region of the search space. This effectively reduces the beam search to a single-path
   search, losing the diversity advantage.

   Solutions include:

   - Using stochastic selection methods
   - Explicitly enforcing diversity through techniques like niching or crowding
   - Periodically reintroducing random states to the beam
   - Adding a penalty term to the objective function that discourages similar states

2. **Computational Efficiency**: Generating and evaluating all neighbors of k states can be computationally expensive,
   especially for problems with large branching factors.

   Solutions include:

   - Implementing efficient data structures for state representation and successor generation
   - Using approximate evaluation functions for initial screening
   - Parallelizing the neighbor generation and evaluation process
   - Implementing beam search with bounded branching factor

3. **Parameter Tuning**: The effectiveness of local beam search depends on parameters like beam width (k), the neighbor
   generation strategy, and selection pressure in stochastic variants.

   Solutions include:

   - Using adaptive parameter tuning based on search progress
   - Implementing anytime algorithms that can return the best solution found so far if interrupted
   - Conducting parameter sensitivity analysis for specific problem domains

4. **Memory Requirements**: Maintaining k states and their successors can require significant memory, especially for
   problems with complex state representations.

   Solutions include:

   - Implementing memory-efficient state representations
   - Using streaming approaches that evaluate successors incrementally
   - Pruning obviously poor successors early in the evaluation process

5. **Termination Criteria**: Determining when to stop the search can be challenging, especially when the optimal
   solution value is unknown.

   Solutions include:

   - Setting a maximum number of iterations
   - Terminating when improvement falls below a threshold for several consecutive iterations
   - Using domain-specific knowledge to recognize when a satisfactory solution has been found

##### Applications in Complex Landscapes

Local beam search excels in complex optimization landscapes where simpler methods like hill climbing tend to get trapped
in local optima. Its ability to maintain multiple diverse search paths while sharing information makes it particularly
valuable in several domains:

1. **Feature Selection in Machine Learning**: When selecting the optimal subset of features for a predictive model,
   local beam search can efficiently explore the combinatorial space of possible feature subsets, identifying groups of
   features that work well together.
2. **Protein Structure Prediction**: Predicting the three-dimensional structure of proteins involves navigating an
   extremely rugged energy landscape. Local beam search can maintain diverse candidate structures while focusing on the
   most energetically favorable configurations.
3. **Natural Language Processing**: In tasks like machine translation or text summarization, beam search is widely used
   to explore different possible output sequences, maintaining multiple partially-completed translations or summaries
   and extending the most promising ones.
4. **Constraint Satisfaction Problems**: For problems like scheduling or resource allocation with many constraints,
   local beam search can effectively explore the space of feasible solutions, focusing on those that best satisfy the
   optimization criteria.
5. **Robotics Planning**: When planning motion paths for robots in complex environments, local beam search can
   simultaneously explore multiple potential paths, adapting quickly when promising routes emerge or obstacles are
   discovered.

In the context of the 8-Queens problem we've discussed previously, local beam search might maintain k different board
configurations, exploring variations of each by moving queens to different positions. Because it explores multiple
configurations simultaneously, it's more likely to find solutions where no queens threaten each other than basic hill
climbing would be.

The versatility of local beam search, combined with its relatively simple implementation and tunable parameters, makes
it a valuable technique in the optimization toolbox, bridging the gap between simple hill climbing and more complex
population-based methods like genetic algorithms.
